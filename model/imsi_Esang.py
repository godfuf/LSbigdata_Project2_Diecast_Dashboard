import pandas as pd
import numpy as np



#### mergeë¥¼ ìœ„í•´ train data ì „ì²˜ë¦¬
train_df = pd.read_csv('../data/train.csv')
# ê²°ì¸¡ì¹˜ í•˜ë‚˜ ìˆëŠ”ê±° ì œê±°
train_df = train_df.dropna(subset=['working'])
# tryshotì œê±°
train_df = train_df.drop(['tryshot_signal'],axis=1)

# =====
# [1] ì»¬ëŸ¼ ë“œë
# =====
train_df.drop(columns=['count','line', 'name', 'mold_name', 'time', 'date','heating_furnace','molten_volume','passorfail'],
               inplace=True)

train_df.info()
# =====
# [2] í˜•ë³€í™˜
# =====
# int
train_df['id'] = train_df['id'].astype(object)

# datetime
train_df['registration_time'] = pd.to_datetime(train_df['registration_time'])

# category
train_df['working'] = train_df['working'].astype('category')
train_df['emergency_stop'] = train_df['emergency_stop'].astype('category')

# mold code ì›í•« ì¸ì½”ë”© -> intê°’ìœ¼ë¡œ ë³€í™˜
train_df = pd.get_dummies(train_df, columns=['mold_code'], drop_first=False)
train_df['mold_code_8412'] = train_df['mold_code_8412'].astype('int')
train_df['mold_code_8573'] = train_df['mold_code_8573'].astype('int')
train_df['mold_code_8600'] = train_df['mold_code_8600'].astype('int')
train_df['mold_code_8722'] = train_df['mold_code_8722'].astype('int')
train_df['mold_code_8917'] = train_df['mold_code_8917'].astype('int')


# KNNë³´ê°„ë²•ì€ ê° ë³€ìˆ˜ê°„ ê°’ì˜ ì°¨ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°’ì„ ì°¾ëŠ”ë° ë”ë¯¸í™”í•˜ë©´ ì²«ë²ˆì§¸ ë²”ì£¼ê°€ ì—†ì–´ì§€ë‹ˆê¹Œ
# ê°’ì´ ìœ¼ë¡œ ê°„ì£¼ë˜ì„œ ì •í™•í•œ ê°’ì„ ì‚°ì¶œí•˜ì§€ ëª»í•  ê²ƒì´ë¼ ìƒê°

from sklearn.impute import KNNImputer
import pandas as pd
train_df.info()
train_df.isnull().sum()
# ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì˜¨ë„ ì»¬ëŸ¼ë“¤
temp_cols_with_missing = [
    'molten_temp',
    'upper_mold_temp3',
    'lower_mold_temp3'
]

# ë³´ê°„ ì‹œ ì‚¬ìš©í•  ì£¼ë³€ ë³€ìˆ˜ë“¤ (ìˆ«ìí˜•, ì˜¨ë„/ìš´ì „ ê´€ë ¨ ë³€ìˆ˜ ìœ„ì£¼)
base_numeric_cols = [
    'facility_operation_cycleTime',
    'production_cycletime', 'EMS_operation_time',
    'mold_code', 'cast_pressure', 'biscuit_thickness',
    'upper_mold_temp1', 'upper_mold_temp2',
    'lower_mold_temp1', 'lower_mold_temp2',
    'Coolant_temperature', 'sleeve_temperature','low_section_speed',
    'high_section_speed','physical_strength','mold_code_8412',
      'mold_code_8573', 'mold_code_8600', 'mold_code_8722',
       'mold_code_8917']


# ë³´ê°„ ìˆ˜í–‰
for temp_col in temp_cols_with_missing:
    if temp_col in train_df.columns:
        missing_count = train_df[temp_col].isna().sum()
        if missing_count > 0:
            # í˜„ì¬ temp_colì„ í¬í•¨í•œ ì»¬ëŸ¼ ì¡°í•©
            available_cols = [col for col in base_numeric_cols + [temp_col] if col in train_df.columns]

            df_temp = train_df[available_cols].copy()

            # KNN ë³´ê°„ê¸° ì´ˆê¸°í™” ë° ì ìš©
            knn_imputer = KNNImputer(n_neighbors=min(5, len(train_df)), weights='uniform')
            df_imputed = pd.DataFrame(
                knn_imputer.fit_transform(df_temp),
                columns=available_cols,
                index=df_temp.index
            )

            # ì›ë³¸ì— ë³´ê°„ëœ ê°’ ë°˜ì˜
            train_df[temp_col] = df_imputed[temp_col]
            print(f"âœ… KNN ë³´ê°„ìœ¼ë¡œ '{temp_col}'ì˜ ê²°ì¸¡ì¹˜ {missing_count}ê°œ ì²˜ë¦¬ ì™„ë£Œ")


train_df.keys()
train_df.isnull().sum()

# í˜„ì‹¤ì ìœ¼ë¡œ ì¼ì–´ë‚  ìˆ˜ ì—†ëŠ” ê·¹ë‹¨ì ì¸ ì´ìƒì¹˜ë§Œ ì œê±°
# ì •ì§€ ë°ì´í„°ê°€ ìˆëŠ”ë° ì´ê±¸ ëº„ê¹Œ ë§ê¹Œ ë°ì´í„°ê°€ ë³„ë¡œ ì—…ì–´ì„œ ë¹¼ë„ ê´œì°®ìŒ 
# ì´ìƒì¹˜ ê·¹ë‹¨ê°’ ì‚­ì œ ê¸°ì¤€ ì…ë ¥ê°’ì˜¤ë¥˜ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•œ ê°’, 10ê°œ ì´í•˜ ê°œìˆ˜

# low_section_speed 65535ê°’ ì œê±°
train_df = train_df[train_df['low_section_speed'] != 65535]

# biscuit_thickness 420 422 ê°’ ì œê±° ì…ë ¥ ì˜¤ë¥˜ë¼ê³  ìƒê° ë‘ìë¦¬ìˆ˜ëŠ” ë‘ê»˜ê°€ ëª°ë“œ ì½”ë“œë³„ë¡œ ë‹¤ë¥¼ìˆ˜ë„ ìˆë‹¤ê³  ìƒê°
train_df['biscuit_thickness'].unique()
train_df['biscuit_thickness'].value_counts()
train_df = train_df[train_df['biscuit_thickness'] != 422]
train_df = train_df[train_df['biscuit_thickness'] != 420]
train_df = train_df[train_df['biscuit_thickness'] >= 10]

# upper_mold_temp1 1449 18~300 í›„ë°˜ 1449 ë§ì•ˆë˜ëŠ”ê°’
train_df['upper_mold_temp1'].sort_values()
train_df = train_df[train_df['upper_mold_temp1'] != 1449]

# upper_mold_temp2  15~389 4232 ë§ì•ˆë˜ëŠ” ê°’
train_df['upper_mold_temp2'].sort_values()
train_df = train_df[train_df['upper_mold_temp2'] != 4232]

# lower_mold_temp3 65503 ë§ì•ˆë˜ëŠ” ê°’
train_df['lower_mold_temp3'].sort_values()
train_df['lower_mold_temp3'].value_counts()
train_df = train_df[train_df['lower_mold_temp3'] != 65503]

# physical_strength ë³´í†µ 600 ~ 700 (65535ì›ë˜ìˆì—ˆëŠ”ë° ë‹¤ë¥¸ê±° ì œì™¸í•˜ë‹¤ ì—†ì–´ì§„ë“¯) 0,2 ë§ì•ˆë˜ëŠ” ê°’ 
train_df['physical_strength'].sort_values()
train_df['physical_strength'].value_counts()
train_df['physical_strength'].unique()
train_df = train_df[train_df['physical_strength'] != 0]
train_df = train_df[train_df['physical_strength'] != 2]

# Coolant_temperature ëƒ‰ê°ìˆ˜ì˜¨ë„ê°€ 1449ë„? ë§ì•ˆë˜ì§€
train_df['Coolant_temperature'].value_counts()
train_df['Coolant_temperature'].unique()
train_df = train_df[train_df['Coolant_temperature'] != 1449]

train_df_E = pd.read_csv('../data/train_ES.csv')
#########################################################

# IQR í™•ì¸
# ì´ìƒì¹˜ ìƒí•œ í•˜í•œ ê¸°ì¤€ ê¸°ë³¸ì ìœ¼ë¡œ IQR 1.5ë¡œ í•˜ë˜ 
# í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì³ì§„ ê·¸ë˜í”„ì¼ ë•Œ ìƒí•œ í•˜í•œ ì¡°ì •í•˜ëŠ” ë°©ì‹
# IQR 1.5ë¡œ í–ˆì„ ë•Œ ì´ìƒì¹˜ê°€ ë§ì´ ì°í˜€ë„ ë‹¤ìŒ ë‹¨ê³„ì— DBSCANì´ë‘ isolation forestë¡œ ì´ìƒì¹˜ í›„ë³´êµ°ì„ 
# ë‹¤ì‹œ íƒì§€í• ê±°ê¸° ë•Œë¬¸ì— ì¼ê´„ì ìœ¼ë¡œ ì¡ì•„ë„ ë ê²ƒ ê°™ë‹¤ê³  ìƒê°


import matplotlib.pyplot as plt
import seaborn as sns

# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ (ì›í•˜ëŠ” ë³€ìˆ˜ë“¤ë§Œ ì„ íƒ ê°€ëŠ¥)
numeric_vars = [
    'molten_temp', 'facility_operation_cycleTime', 'production_cycletime', 
    'low_section_speed', 'high_section_speed', 'cast_pressure', 'biscuit_thickness',
    'upper_mold_temp1', 'upper_mold_temp2', 'upper_mold_temp3', 
    'lower_mold_temp1', 'lower_mold_temp2', 'lower_mold_temp3', 
    'sleeve_temperature', 'physical_strength', 'Coolant_temperature','EMS_operation_time'
]

for var in numeric_vars:
    plt.figure(figsize=(12,6))
    sns.boxplot(x='mold_code', y=var, data=train_df_E)
    plt.title(f'Mold Codeë³„ {var} ë¶„í¬')
    plt.xlabel('Mold Code')
    plt.ylabel(var)
    plt.grid(True)
    plt.show()


# mold code ë³„ ë¶„ì„
train_df_E = pd.read_csv('../data/train_ES.csv')

# mold code 8412  18114ê°œ
# IQR 
train_8412 = train_df_E[train_df_E['mold_code']==8412]

# molten_temp
Q1 = train_8412['molten_temp'].quantile(0.25)
Q3 = train_8412['molten_temp'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5*IQR
upper_bound = Q3 + 1.5*IQR

normal = ((train_8412['molten_temp'] > lower_bound) & (train_8412['molten_temp'] < upper_bound)).sum()
outliers = ((train_8412['molten_temp'] <= lower_bound) | (train_8412['molten_temp'] >= upper_bound)).sum()




# facility_operation_cycleTime ì½”ë“œ ë³„ë¡œ IQR 1.5ë¡œ ëŠê¸°
Q1 = train_8412['facility_operation_cycleTime'].quantile(0.25)
Q3 = train_8412['facility_operation_cycleTime'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5*IQR
upper_bound = Q3 + 1.5*IQR

normal = ((train_8412['facility_operation_cycleTime'] > lower_bound) & (train_8412['facility_operation_cycleTime'] < upper_bound)).sum()
outliers = ((train_8412['facility_operation_cycleTime'] <= lower_bound) | (train_8412['facility_operation_cycleTime'] >= upper_bound)).sum()





###################################### ì´ìƒì¹˜###################################
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN

# IQR
# train_df_E = pd.read_csv('../data/train_ES.csv')
# # IQR ì´ìƒì¹˜ 1.5 ìƒí•œ í•˜í•œ í•˜ê³  ê·¸ë¬ì„ ë•Œ ë³€ìˆ˜ ë³„ ì´ìƒì¹˜ ë¹„ìœ¨
# def code_IQR():
#     numeric_vars = [
#         'molten_temp', 'facility_operation_cycleTime', 'production_cycletime', 
#         'low_section_speed', 'high_section_speed', 'cast_pressure', 'biscuit_thickness',
#         'upper_mold_temp1', 'upper_mold_temp2', 'upper_mold_temp3', 
#         'lower_mold_temp1', 'lower_mold_temp2', 'lower_mold_temp3', 
#         'sleeve_temperature', 'physical_strength', 'Coolant_temperature', 'EMS_operation_time'
#     ]

#     result = []

#     for i in train_df_E['mold_code'].unique():
#         mold_data = train_df_E[train_df_E['mold_code'] == i]
#         for var in numeric_vars:
#             Q1 = mold_data[var].quantile(0.25)
#             Q3 = mold_data[var].quantile(0.75)
#             IQR = Q3 - Q1
#             lower_bound = Q1 - IQR * 1.5
#             upper_bound = Q3 + IQR * 1.5
#             outliers = (mold_data[var] < lower_bound) | (mold_data[var] > upper_bound)
#             outlier_ratio = outliers.sum() / len(mold_data)

#             result.append({
#                 'mold_code': i,
#                 'variable': var,
#                 'outlier_ratio(%)': round(outlier_ratio * 100, 2)
#             })

#     return result



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN

# DBSCAN ì´ìƒì¹˜ í›„ë³´ ì¤‘ ì´ìƒì¹˜ (êµ°ì§‘ìˆëŠ”ê±´ íŒ¨í„´ì´ ìˆëŠ” ì´ìƒì¹˜(í™•ì¸í•„ìš”), êµ°ì§‘ì—†ëŠ” ì´ìƒì¹˜ëŠ” í•œë²ˆì”© íŠ€ëŠ” ë¶ˆê·œì¹™ì ì¸ ì´ìƒì¹˜ë¼ê³  ìƒê°)
# IQR 1.5ë¡œí•˜ê³   PCA -> IQR -> DBSCAN -> ì‹œê°í™”
# IQRë¡œ ì´ìƒì¹˜ í›„ë³´êµ°ì„ í•œë²ˆ ê±¸ëŸ¬ë‚´ê³  ë˜ DBSCANìœ¼ë¡œ ì´ìƒì¹˜ 
train_df_E = pd.read_csv('../data/train_ES.csv') # ëª°ë“œ ì½”ë“œ ì›í•«ì¸ì½”ë”©
train_df_E = train_df_E.drop(columns=['Unnamed: 0'])
train_df_E = train_df_E.drop(columns=['id'])
train_df_E['mold_code'] = train_df_E['mold_code'].astype('object')
train_df_E2 = train_df_E.copy()
train_df_E2.info()
# 1. ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ì¶”ì¶œ (float64, int64)
numeric_cols = train_df_E2.select_dtypes(include=['float64', 'int64']).columns
X_numeric = train_df_E2[numeric_cols]

# 2. ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numeric)

# 3. PCA (2ê°œ ì£¼ì„±ë¶„)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)


# 5. PCA ê²°ê³¼ DataFrame ìƒì„±
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])

# 6. IQR ê¸°ì¤€ ì´ìƒì¹˜ íƒì§€ í•¨ìˆ˜
def find_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = series[(series < lower_bound) | (series > upper_bound)].index
    return outliers

outliers_PC1 = find_outliers(df_pca['PC1'])
outliers_PC2 = find_outliers(df_pca['PC2'])

# 7. ì´ìƒì¹˜ í›„ë³´ ì¸ë±ìŠ¤ í•©ì§‘í•©
outlier_indices = list(set(outliers_PC1) | set(outliers_PC2))
print(f"Detected outliers count: {len(outlier_indices)}")

# 8. ì´ìƒì¹˜ í›„ë³´ ë°ì´í„°ë§Œ ì¶”ì¶œ
X_outliers = df_pca.loc[outlier_indices]


# 9. DBSCAN í´ëŸ¬ìŠ¤í„°ë§
dbscan = DBSCAN(eps=0.6, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_outliers)
print("Estimated number of clusters: " ,len(np.unique(dbscan_labels[dbscan_labels!=-1])))
print("Estimated number of noise points: ",len(dbscan_labels[dbscan_labels==-1]))

# 10. DBSCAN ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(8, 6))
unique_labels = set(dbscan_labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):
    class_member_mask = (dbscan_labels == k)
    xy = X_outliers[class_member_mask]

    if k == -1:
        col = [0, 0, 0, 1]  # Noise = ê²€ì •ìƒ‰

    plt.scatter(xy['PC1'], xy['PC2'], c=[col], label=f'Cluster {k}' if k != -1 else 'Noise', alpha=0.6)

plt.title('DBSCAN Clustering on PCA Outliers')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()


#################################3isolation forestë¡œ ë½‘ì€ ì •ìƒì¹˜ì˜ IQRìƒí•œ í•˜í•œìœ¼ë¡œ ì´ìƒì¹˜ ë½‘ì•„ì„œ
# DBSCANëŒë¦°ê±°       isolation -> IQR(isolation) -> PCA -> DBSCAN -> ì‹œê°í™”
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train_df_E = pd.read_csv('../data/train_ES.csv')
train_df_E = train_df_E.drop(columns=['Unnamed: 0'])
train_df_E = train_df_E.drop(columns=['id'])
train_df_E['mold_code'] = train_df_E['mold_code'].astype('object')
train_df_E3 = train_df_E.copy()

# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì„ íƒ
numeric_cols = train_df_E3.select_dtypes(include=['float64', 'int64']).columns
X_numeric = train_df_E3[numeric_cols]

# í‘œì¤€í™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numeric)

# Isolation Forestë¡œ ì •ìƒ ë°ì´í„° ì„ ë³„
iso = IsolationForest(contamination=0.06, random_state=42)
iso_labels = iso.fit_predict(X_scaled)  # ì •ìƒ: 1, ì´ìƒì¹˜: -1

normal_data = train_df_E3[iso_labels == 1]  # ì •ìƒ ë°ì´í„°ë§Œ ì¶”ì¶œ

# âœ… IQR ìƒí•˜í•œ ê³„ì‚° í•¨ìˆ˜ (mold_code ê¸°ì¤€)
def normal_IQR(normal_data, mold_col='mold_code'):
    numeric_vars = [
        'molten_temp', 'facility_operation_cycleTime', 'production_cycletime', 
        'low_section_speed', 'high_section_speed', 'cast_pressure', 'biscuit_thickness',
        'upper_mold_temp1', 'upper_mold_temp2', 'upper_mold_temp3', 
        'lower_mold_temp1', 'lower_mold_temp2', 'lower_mold_temp3', 
        'sleeve_temperature', 'physical_strength', 'Coolant_temperature', 'EMS_operation_time'
    ]
    
    bounds = []

    for code in normal_data[mold_col].unique():
        mold_data = normal_data[normal_data[mold_col] == code]
        for var in numeric_vars:
            Q1 = mold_data[var].quantile(0.25)
            Q3 = mold_data[var].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            bounds.append({
                'mold_code': code,
                'variable': var,
                'lower_bound': lower_bound,
                'upper_bound': upper_bound
            })
    
    return pd.DataFrame(bounds)

# IQR ìƒí•˜í•œ ê¸°ì¤€ ì–»ê¸°
iqr_bounds_df = normal_IQR(normal_data)

# âœ… IQR ê¸°ì¤€ ì´ìƒì¹˜ íŒë³„ í•¨ìˆ˜
def is_iqr_outlier(row, bounds_df, mold_col='mold_code'):
    mold = row[mold_col]
    for _, bound in bounds_df[bounds_df['mold_code'] == mold].iterrows():
        var = bound['variable']
        val = row[var]
        if val < bound['lower_bound'] or val > bound['upper_bound']:
            return True  # í•˜ë‚˜ë¼ë„ ë²—ì–´ë‚˜ë©´ ì´ìƒì¹˜ë¡œ ê°„ì£¼
    return False

# ì „ì²´ ë°ì´í„° ì¤‘ IQR ë²—ì–´ë‚œ ì´ìƒì¹˜ë§Œ í•„í„°ë§
iqr_outliers = train_df_E3[train_df_E3.apply(lambda row: is_iqr_outlier(row, iqr_bounds_df), axis=1)]
print(f"IQR ê¸°ë°˜ ì´ìƒì¹˜ ìˆ˜: {len(iqr_outliers)}")

# PCAë¥¼ ìœ„í•œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì¶”ì¶œ ë° í‘œì¤€í™”
X_iqr_outliers = iqr_outliers[numeric_cols]
X_iqr_scaled = scaler.transform(X_iqr_outliers)  # ê°™ì€ scaler ì‚¬ìš©

# PCA (2ê°œ ì£¼ì„±ë¶„)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_iqr_scaled)
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])

# âœ… DBSCAN í´ëŸ¬ìŠ¤í„°ë§
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(df_pca)

print("Estimated number of clusters:", len(set(dbscan_labels) - {-1}))
print("Estimated number of noise points:", list(dbscan_labels).count(-1))

# âœ… ì‹œê°í™”
plt.figure(figsize=(8, 6))
unique_labels = set(dbscan_labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):
    class_mask = (dbscan_labels == k)
    cluster_points = df_pca[class_mask]

    if k == -1:
        col = [0, 0, 0, 1]  # Noise = ê²€ì •ìƒ‰

    plt.scatter(cluster_points['PC1'], cluster_points['PC2'], c=[col], label=f'Cluster {k}' if k != -1 else 'Noise', alpha=0.6)

plt.title('DBSCAN Clustering on IQR Outliers (based on ISO-normal data)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()




#####################  PCA -> DBSCAN -> ì‹œê°í™”
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬
train_df_E = pd.read_csv('../data/train_ES.csv')
train_df_E = train_df_E.drop(columns=['Unnamed: 0'])
train_df_E = train_df_E.drop(columns=['id'])
train_df_E['mold_code'] = train_df_E['mold_code'].astype('object')

# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì¶”ì¶œ
numeric_cols = train_df_E.select_dtypes(include=['float64', 'int64']).columns
X_numeric = train_df_E[numeric_cols]

# í‘œì¤€í™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numeric)

# PCA 2ì°¨ì› ì¶•ì†Œ
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])

# DBSCAN í´ëŸ¬ìŠ¤í„°ë§ (IQR ì œê±°)
dbscan = DBSCAN(eps=0.5, min_samples=11)
dbscan_labels = dbscan.fit_predict(df_pca)

# í´ëŸ¬ìŠ¤í„° ìˆ˜ ë° ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ìˆ˜ ì¶œë ¥
print("Estimated number of clusters:", len(set(dbscan_labels) - {-1}))
print("Estimated number of noise points:", list(dbscan_labels).count(-1))

# ì‹œê°í™”
plt.figure(figsize=(8, 6))
unique_labels = set(dbscan_labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):
    class_mask = (dbscan_labels == k)
    cluster_points = df_pca[class_mask]
    
    if k == -1:
        col = [0, 0, 0, 1]  # Noise = ê²€ì •ìƒ‰

    plt.scatter(cluster_points['PC1'], cluster_points['PC2'], c=[col], label=f'Cluster {k}' if k != -1 else 'Noise', alpha=0.6)

plt.title('DBSCAN Clustering on PCA Data (No IQR)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()



##########################3ì „ì²´ ë°ì´í„° isolation forest#####
# ë¶ˆëŸ‰ë¥  ì „ì²´ ì•½4.4% ì‹¤ì œ ë¶ˆëŸ‰ë¥  ë³´ë‹¤ ì•½ê°„ì˜ ì—¬ìœ ë¥¼ ë‘ê³ ì 6%ë¡œ ì„¤ì •

train_df_E = pd.read_csv('../data/train_ES.csv')
train_df_E = train_df_E.drop(columns=['Unnamed: 0','id'])
train_df_E['mold_code'] = train_df_E['mold_code'].astype('object')

from sklearn.ensemble import IsolationForest 

train_df_E3 = train_df_E.copy()
X = train_df_E3.select_dtypes(include=np.number) 
X.keys()
# ìŠ¤ì¼€ì¼ë§
X_scaled = StandardScaler().fit_transform(X)

iso_forest = IsolationForest(n_estimators=150, contamination=0.06, random_state=42)
iso_labels = iso_forest.fit_predict(X_scaled)

print("Isolation Forest ì´ìƒì¹˜ ìˆ˜:", sum(iso_labels == -1))
print("Isolation Forest ì •ìƒ ìˆ˜ :", sum(iso_labels == 1))
# ì´ìƒì¹˜ ìˆ˜: 4385
# ì •ìƒ ìˆ˜: 68696

# PCA 2D ì‹œê°í™”
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
pca_df['anomaly'] = iso_labels



# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(8, 6))
colors = ['red' if label == -1 else 'green' for label in iso_labels]
plt.scatter(pca_df['PC1'], pca_df['PC2'], c=colors, alpha=0.6)
plt.title('Isolation Forest on PCA Inliers (IQR ì•ˆìª½ ë°ì´í„°)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()



 
 ## isolation forestë¡œ ì´ìƒì¹˜ê°€ ì•„ë‹Œ ìƒ˜í”Œë¡œ IQR ìƒí•œí•˜í•œ ì •í•˜ê¸°

normal_data = train_df_E3[iso_labels==1] # traindataì™€ ìŠ¤ì¼€ì¼ í•œ X_scaledë‘ ë°°ì—´ì´ ê°™ê¸°ë•Œë¬¸ì— ì´ëŸ°ì‹ì„ í•„í„°ë§ ê°€ëŠ¥

normal_data


def normal_IQR(normal_data, mold_col='mold_code'):
    numeric_vars = [
        'molten_temp', 'facility_operation_cycleTime', 'production_cycletime', 
        'low_section_speed', 'high_section_speed', 'cast_pressure', 'biscuit_thickness',
        'upper_mold_temp1', 'upper_mold_temp2', 'upper_mold_temp3', 
        'lower_mold_temp1', 'lower_mold_temp2', 'lower_mold_temp3', 
        'sleeve_temperature', 'physical_strength', 'Coolant_temperature', 'EMS_operation_time'
    ]
    
    bounds = []

    for code in normal_data[mold_col].unique():
        mold_data = normal_data[normal_data[mold_col] == code]
        for var in numeric_vars:
            Q1 = mold_data[var].quantile(0.25)
            Q3 = mold_data[var].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            bounds.append({
                'mold_code': code,
                'variable': var,
                'Q1': Q1,
                'Q3': Q3,
                'IQR': IQR,
                'lower_bound': lower_bound,
                'upper_bound': upper_bound
            })
    
    return pd.DataFrame(bounds)


################# isolation forest
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
import joblib

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
train_df_E = pd.read_csv('../data/train_ES.csv')
train_df_E = train_df_E.drop(columns=['Unnamed: 0', 'id'])
train_df_E['mold_code'] = train_df_E['mold_code'].astype('object')
train_df_E.keys()
# 2. ìˆ˜ì¹˜í˜• ë°ì´í„° ì„ íƒ
X = train_df_E.select_dtypes(include=np.number)

# 3. ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Isolation Forest ëª¨ë¸ í•™ìŠµ
iso_forest = IsolationForest(n_estimators=150, contamination=0.06, random_state=42)
iso_forest.fit(X_scaled)

# 5. PCA 2ì°¨ì› í•™ìŠµ
pca = PCA(n_components=2)
pca.fit(X_scaled)

# 6. ëª¨ë¸ ì €ì¥
joblib.dump(scaler, "scaler.pkl")
joblib.dump(iso_forest, "isolation_model.pkl")
joblib.dump(pca, "pca_model.pkl")

print("âœ… Scaler, IsolationForest, PCA ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

#
import pandas as pd
train_df_E

import pandas as pd

# ì‚¬ìš©í•  ì„¼ì„œ ì»¬ëŸ¼ë“¤
sensor_columns = [
    'molten_temp', 'facility_operation_cycleTime', 'production_cycletime', 
    'low_section_speed', 'high_section_speed', 'cast_pressure', 
    'biscuit_thickness', 'upper_mold_temp1', 'upper_mold_temp2', 
    'upper_mold_temp3', 'lower_mold_temp1', 'lower_mold_temp2', 
    'lower_mold_temp3', 'sleeve_temperature', 'physical_strength', 
    'Coolant_temperature'
]

# ì–‘í’ˆ ë°ì´í„°ë§Œ í•„í„°ë§ (passorfail == 0)
good_data = train_df_E[train_df_E['passorfail'] == 0]

print(f"ì „ì²´ ë°ì´í„°: {len(train_df_E)}ê±´")
print(f"ì–‘í’ˆ ë°ì´í„°: {len(good_data)}ê±´ ({len(good_data)/len(train_df_E)*100:.1f}%)")
print()

# ëª°ë“œë³„ ì–‘í’ˆ í‰ê·  ê³„ì‚°
mold_good_averages = {}
mold_codes = sorted(train_df_E['mold_code'].unique())

for mold_code in mold_codes:
    mold_good_data = good_data[good_data['mold_code'] == mold_code]
    
    print(f"ğŸ”§ ëª°ë“œ {mold_code}: ì–‘í’ˆ {len(mold_good_data)}ê±´")
    
    if len(mold_good_data) > 0:
        mold_averages = {}
        for sensor in sensor_columns:
            if sensor in mold_good_data.columns:
                avg_value = mold_good_data[sensor].mean()
                mold_averages[sensor] = round(avg_value, 2)
                print(f"  {sensor}: {avg_value:.2f}")
        
        mold_good_averages[str(mold_code)] = mold_averages
    else:
        print(f"  âš ï¸ ì–‘í’ˆ ë°ì´í„° ì—†ìŒ")
    print()

# ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì¶œë ¥ (ì½”ë“œì— ì§ì ‘ ë³µì‚¬ ê°€ëŠ¥)
print("="*60)
print("ğŸ“‹ ì½”ë“œì— ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœ:")
print("="*60)
print("DEFECT_NORMAL_PATTERNS = {")

for mold_code, averages in mold_good_averages.items():
    print(f'    "{mold_code}": {{')
    for sensor, value in averages.items():
        print(f'        "{sensor}": {value},')
    print("    },")

print("}")

# ê° ëª°ë“œë³„ í†µê³„ ìš”ì•½
print("\n" + "="*60)
print("ğŸ“Š ëª°ë“œë³„ ì–‘í’ˆ ë°ì´í„° í†µê³„:")
print("="*60)
for mold_code in mold_codes:
    mold_good_count = len(good_data[good_data['mold_code'] == mold_code])
    mold_total_count = len(train_df_E[train_df_E['mold_code'] == mold_code])
    good_rate = mold_good_count / mold_total_count * 100 if mold_total_count > 0 else 0
    
    print(f"ëª°ë“œ {mold_code}: ì–‘í’ˆ {mold_good_count}/{mold_total_count}ê±´ ({good_rate:.1f}%)")

# í•¨ìˆ˜ í˜•íƒœë¡œë„ ì œê³µ
print("\n" + "="*60)
print("ğŸ”§ í•¨ìˆ˜ í˜•íƒœ (paste-4.txtì— ì¶”ê°€):")
print("="*60)

function_code = '''
def get_defect_normal_pattern(mold_code):
    """ë¶ˆëŸ‰ ì˜ˆì¸¡ìš© ì–‘í’ˆ ê¸°ì¤€ê°’ - ì‹¤ì œ ì–‘í’ˆ ë°ì´í„° ê¸°ë°˜"""
    
    DEFECT_NORMAL_PATTERNS = {
'''

for mold_code, averages in mold_good_averages.items():
    function_code += f'        "{mold_code}": {{\n'
    for sensor, value in averages.items():
        function_code += f'            "{sensor}": {value},\n'
    function_code += "        },\n"

function_code += '''    }
    
    pattern = DEFECT_NORMAL_PATTERNS.get(str(mold_code))
    
    if pattern:
        return pattern
    else:
        print(f"ğŸš¨ [WARNING] ëª°ë“œ {mold_code} ì–‘í’ˆ íŒ¨í„´ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
        # ì „ì²´ ëª°ë“œ í‰ê·  ë°˜í™˜
        return DEFECT_NORMAL_PATTERNS.get("8412", {})  # ê¸°ë³¸ê°’
'''

print(function_code)