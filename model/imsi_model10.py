"""
XGBoost/LightGBM ì „ìš© ìµœì í™” íŒŒì´í”„ë¼ì¸ (ìµœì¢… ëª¨ë¸ êµ¬ì„±)
XGBoost/LightGBM ìì²´ íŠœë‹ + ì •ë°€ë„ ì œì•½ ì„ê³„ê°’ ìµœì í™”
EMS_operation_time ì™„ì „ ì œê±°ë¡œ ì•ˆì •ì„± í™•ë³´
ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ í•˜ì—ì„œ ë¯¼ê°ë„ ìµœëŒ€í™”
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, f1_score, recall_score, precision_score, accuracy_score
from sklearn.impute import KNNImputer
import xgboost as xgb
import lightgbm as lgb
import pickle
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("         XGBoost/LightGBM ì „ìš© ìµœì í™” íŒŒì´í”„ë¼ì¸")
print("       ìì²´ íŠœë‹ + ì •ë°€ë„ ì œì•½ ì„ê³„ê°’ ìµœì í™”")
print("       ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ í•˜ì—ì„œ ë¯¼ê°ë„ ìµœëŒ€í™”")
print("=" * 80)

class MoldCodeQualityPipeline:
    """ì œì¡°ì—… í’ˆì§ˆ ì˜ˆì¸¡ í†µí•© íŒŒì´í”„ë¼ì¸ (XGBoost/LightGBM ì „ìš©)"""
    
    def __init__(self):
        self.models = {}
        self.best_params = {}
        self.optimal_thresholds = {}
        self.feature_names = None
        
        # ê° mold_codeë³„ ìµœì  ëª¨ë¸ ì„¤ì • (ìµœì‹  ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)
        self.best_model_config = {
            8412: 'XGBoost_Balanced',
            8573: 'XGBoost_Balanced', 
            8600: 'LightGBM_Balanced',  # GradientBoosting ëŒ€ì‹  LightGBM ì‚¬ìš©
            8722: 'XGBoost_Balanced',
            8917: 'LightGBM_Balanced'
        }
        
        print("âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ê° mold_codeë³„ ìµœì  ëª¨ë¸: {len(self.best_model_config)}ê°œ")
        print(f"   - XGBoost: 3ê°œ (8412, 8573, 8722)")
        print(f"   - LightGBM: 2ê°œ (8600, 8917)")
        print(f"   - EMS_operation_time ì œê±°: ì•ˆì •ì„± í™•ë³´")
        print(f"   - GridSearchCV ì œê±°: ìì²´ íŠœë‹ìœ¼ë¡œ ê³ ì†í™”")
        print(f"   - SMOTE ì œê±°: ë‚´ì¥ ë¶ˆê· í˜• ì²˜ë¦¬ë¡œ ë‹¨ìˆœí™”")
        print(f"   ğŸ¯ ì„ê³„ê°’ ì „ëµ: ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ì—ì„œ ë¯¼ê°ë„ ìµœëŒ€í™”")
    
    def preprocess_data(self, df_raw, is_training=True):
        """ë°ì´í„° ì „ì²˜ë¦¬ (EMS_operation_time ì™„ì „ ì œê±°)"""
        
        print(f"\nğŸ”§ ì „ì²˜ë¦¬ ì‹œì‘ (í›ˆë ¨ ëª¨ë“œ: {is_training})")
        
        # ì›ë³¸ ë°ì´í„° ë³µì‚¬
        df = df_raw.copy()
        
        # tryshot_signal ë¹„ì¦ˆë‹ˆìŠ¤ ë£° ì²´í¬
        tryshot_exists = df['tryshot_signal'].notna()
        tryshot_count = tryshot_exists.sum()
        
        if tryshot_count > 0:
            print(f"ğŸš¨ tryshot_signal ê°’ì´ ìˆëŠ” ë°ì´í„° {tryshot_count}ê°œ ë°œê²¬")
            print("   â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ë£°ì— ë”°ë¼ ë¬´ì¡°ê±´ ë¶ˆëŸ‰(1)ë¡œ ì˜ˆì¸¡")
            
            # tryshotì´ ìˆëŠ” ë°ì´í„°ëŠ” ë³„ë„ ì²˜ë¦¬ (ë¬´ì¡°ê±´ ë¶ˆëŸ‰)
            tryshot_data = df[tryshot_exists].copy()
            tryshot_predictions = pd.Series(1, index=tryshot_data.index, name='prediction')
            
            # tryshotì´ ì—†ëŠ” ë°ì´í„°ë§Œ ëª¨ë¸ ì˜ˆì¸¡ ëŒ€ìƒ
            df = df[~tryshot_exists].copy()
            print(f"   â†’ ëª¨ë¸ ì˜ˆì¸¡ ëŒ€ìƒ: {len(df)}ê°œ")
        else:
            print("âœ… tryshot_signal ê°’ì´ ì—†ìŒ - ëª¨ë“  ë°ì´í„° ëª¨ë¸ ì˜ˆì¸¡")
            tryshot_data = None
            tryshot_predictions = None
        
        # ê¸°ë³¸ í•„í„°ë§ ë° ë³€ìˆ˜ ì œê±° (EMS_operation_time í¬í•¨!)
        exclude_columns = ['id', 'line', 'name', 'mold_name', 'time', 'date', 
                          'emergency_stop', 'molten_volume', 'registration_time', 
                          'heating_furnace', 'count', 'tryshot_signal', 'EMS_operation_time']
        
        df = df.drop(columns=[col for col in exclude_columns if col in df.columns])
        print(f"   ğŸš« EMS_operation_time ì œê±°ë¡œ ì•ˆì •ì„± í™•ë³´")
        
        # working ê²°ì¸¡ì¹˜ ì œê±°
        if 'working' in df.columns and df['working'].isnull().sum() > 0:
            df = df.dropna(subset=['working'])
        
        # working ë ˆì´ë¸” ì¸ì½”ë”©ë§Œ ìˆ˜í–‰ (EMS_operation_time ì œì™¸!)
        if is_training:
            self.le_working = LabelEncoder()
            
            if 'working' in df.columns:
                df['working'] = self.le_working.fit_transform(df['working'])
                print(f"   âœ… working ë ˆì´ë¸” ì¸ì½”ë”© ì™„ë£Œ")
        else:
            # ì˜ˆì¸¡ì‹œì—ëŠ” ê¸°ì¡´ ì¸ì½”ë” ì‚¬ìš©
            if 'working' in df.columns and hasattr(self, 'le_working'):
                try:
                    df['working'] = self.le_working.transform(df['working'])
                except ValueError as e:
                    print(f"âš ï¸ working ì¸ì½”ë”© ì˜¤ë¥˜: {str(e)}")
                    # ìƒˆë¡œìš´ ê°’ì´ ìˆìœ¼ë©´ í›ˆë ¨ì‹œ ê°€ì¥ ë¹ˆë²ˆí–ˆë˜ ê°’ìœ¼ë¡œ ëŒ€ì²´
                    unknown_mask = ~df['working'].isin(self.le_working.classes_)
                    if unknown_mask.any():
                        first_class = self.le_working.classes_[0]
                        print(f"   - ìƒˆë¡œìš´ working ê°’ë“¤ì„ '{first_class}'ë¡œ ëŒ€ì²´: {unknown_mask.sum()}ê°œ")
                        df.loc[unknown_mask, 'working'] = first_class
                    df['working'] = self.le_working.transform(df['working'])
        
        # KNN ë³´ê°„ (í™ë‹˜ ì§€ì • ë³€ìˆ˜ë“¤ë§Œ, EMS_operation_time ì œì™¸!)
        target_impute_cols = ['molten_temp', 'lower_mold_temp3', 'upper_mold_temp3']
        actual_missing = [col for col in target_impute_cols if col in df.columns and df[col].isnull().sum() > 0]
        
        if actual_missing:
            print(f"   - KNN ë³´ê°„ ëŒ€ìƒ: {actual_missing}")
            
            # ì°¸ì¡° ë³€ìˆ˜ì—ì„œ EMS_operation_time ì™„ì „ ì œê±°!
            reference_cols = ['working', 'molten_temp', 'facility_operation_cycleTime', 
                             'production_cycletime', 'mold_code',
                             'lower_mold_temp1', 'lower_mold_temp2', 'lower_mold_temp3',
                             'upper_mold_temp1', 'upper_mold_temp2', 'upper_mold_temp3',
                             'sleeve_temperature', 'Coolant_temperature']
            
            available_reference_cols = [col for col in reference_cols if col in df.columns]
            print(f"   ğŸ“‹ KNN ì°¸ì¡° ë³€ìˆ˜: {len(available_reference_cols)}ê°œ (EMS_operation_time ì œì™¸)")
            
            if is_training:
                self.knn_imputer = KNNImputer(n_neighbors=5, weights='uniform')
                df_temp = df[available_reference_cols].copy()
                df_imputed = pd.DataFrame(
                    self.knn_imputer.fit_transform(df_temp),
                    columns=available_reference_cols,
                    index=df_temp.index
                )
            else:
                df_temp = df[available_reference_cols].copy()
                df_imputed = pd.DataFrame(
                    self.knn_imputer.transform(df_temp),
                    columns=available_reference_cols,
                    index=df_temp.index
                )
            
            # ì§€ì •ëœ ë³€ìˆ˜ë“¤ë§Œ ì—…ë°ì´íŠ¸
            for col in actual_missing:
                df[col] = df_imputed[col]
                print(f"     âœ… {col} KNN ë³´ê°„ ì™„ë£Œ")
        
        # production_cycletime 0ê°’ ì²˜ë¦¬
        if 'production_cycletime' in df.columns:
            zero_count = (df['production_cycletime'] == 0).sum()
            if zero_count > 0:
                if is_training:
                    self.production_mean = df[(df['production_cycletime'] > 0) & 
                                            (df['production_cycletime'] <= 115)]['production_cycletime'].mean()
                
                df.loc[df['production_cycletime'] == 0, 'production_cycletime'] = self.production_mean
                print(f"   - production_cycletime 0ê°’ {zero_count}ê°œë¥¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´")
        
        # ë‚˜ë¨¸ì§€ ê²°ì¸¡ì¹˜ ì œê±°
        before_rows = len(df)
        df = df.dropna()
        removed_rows = before_rows - len(df)
        if removed_rows > 0:
            print(f"   - ë‚˜ë¨¸ì§€ ê²°ì¸¡ì¹˜ ì œê±°: {removed_rows}í–‰ ì œê±°")
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}í–‰")
        
        return df, tryshot_data, tryshot_predictions
    
    def optimize_xgboost(self, X_train, y_train, X_val, y_val):
        """XGBoost ìì²´ íŠœë‹"""
        
        print(f"     ğŸš€ XGBoost ìì²´ íŠœë‹ ì‹œì‘...")
        
        # DMatrix ìƒì„±
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        base_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'scale_pos_weight': 3,  # ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬
            'random_state': 42,
            'verbosity': 0
        }
        
        # íŒŒë¼ë¯¸í„° í›„ë³´ë“¤
        param_candidates = [
            {**base_params, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8},
            {**base_params, 'max_depth': 9, 'learning_rate': 0.1, 'subsample': 0.8},
            {**base_params, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.9},
            {**base_params, 'max_depth': 9, 'learning_rate': 0.05, 'subsample': 0.8}
        ]
        
        best_score = 0
        best_params = None
        best_model = None
        
        # ê° íŒŒë¼ë¯¸í„° ì¡°í•© í…ŒìŠ¤íŠ¸
        for i, params in enumerate(param_candidates):
            try:
                # ì¡°ê¸°ì¢…ë£Œë¡œ ë¹ ë¥¸ í›ˆë ¨
                model = xgb.train(
                    params,
                    dtrain,
                    num_boost_round=200,  # ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •
                    evals=[(dval, 'eval')],
                    early_stopping_rounds=20,  # ì¡°ê¸°ì¢…ë£Œ
                    verbose_eval=False
                )
                
                # ê²€ì¦ ì ìˆ˜ í™•ì¸
                score = model.best_score
                
                if score > best_score:
                    best_score = score
                    best_params = params
                    best_model = model
                    
            except Exception as e:
                print(f"       íŒŒë¼ë¯¸í„° {i+1} ì‹¤íŒ¨: {str(e)[:30]}...")
                continue
        
        # ìµœê³  ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ìƒì„±
        if best_model is None:
            print(f"       âš ï¸ ëª¨ë“  íŒŒë¼ë¯¸í„° ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            best_model = xgb.train(base_params, dtrain, num_boost_round=100)
            best_params = base_params
        
        print(f"       âœ… ìµœê³  ì ìˆ˜: {best_score:.4f}")
        
        return best_model, best_params
    
    def optimize_lightgbm(self, X_train, y_train, X_val, y_val):
        """LightGBM ìì²´ íŠœë‹"""
        
        print(f"     ğŸš€ LightGBM ìì²´ íŠœë‹ ì‹œì‘...")
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        base_params = {
            'objective': 'binary',
            'metric': 'auc',
            'is_unbalance': True,  # ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ (LightGBM ë°©ì‹)
            'random_state': 42,
            'verbosity': -1,
            'force_col_wise': True
        }
        
        # íŒŒë¼ë¯¸í„° í›„ë³´ë“¤
        param_candidates = [
            {**base_params, 'max_depth': 6, 'learning_rate': 0.1, 'num_leaves': 63},
            {**base_params, 'max_depth': 9, 'learning_rate': 0.1, 'num_leaves': 127},
            {**base_params, 'max_depth': 6, 'learning_rate': 0.2, 'num_leaves': 31},
            {**base_params, 'max_depth': -1, 'learning_rate': 0.05, 'num_leaves': 63}
        ]
        
        best_score = 0
        best_params = None
        best_model = None
        
        # ê° íŒŒë¼ë¯¸í„° ì¡°í•© í…ŒìŠ¤íŠ¸
        for i, params in enumerate(param_candidates):
            try:
                # Dataset ìƒì„±
                train_data = lgb.Dataset(X_train, label=y_train)
                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
                
                # ì¡°ê¸°ì¢…ë£Œë¡œ ë¹ ë¥¸ í›ˆë ¨
                model = lgb.train(
                    params,
                    train_data,
                    num_boost_round=200,  # ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •
                    valid_sets=[val_data],
                    callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]  # ì¡°ê¸°ì¢…ë£Œ
                )
                
                # ê²€ì¦ ì ìˆ˜ í™•ì¸
                score = model.best_score['valid_0']['auc']
                
                if score > best_score:
                    best_score = score
                    best_params = params
                    best_model = model
                    
            except Exception as e:
                print(f"       íŒŒë¼ë¯¸í„° {i+1} ì‹¤íŒ¨: {str(e)[:30]}...")
                continue
        
        # ìµœê³  ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ìƒì„±
        if best_model is None:
            print(f"       âš ï¸ ëª¨ë“  íŒŒë¼ë¯¸í„° ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            train_data = lgb.Dataset(X_train, label=y_train)
            best_model = lgb.train(base_params, train_data, num_boost_round=100)
            best_params = base_params
        
        print(f"       âœ… ìµœê³  ì ìˆ˜: {best_score:.4f}")
        
        return best_model, best_params
    
    def optimize_threshold(self, model, X_val, y_val, model_type, mold_code):
        """ì„ê³„ê°’ ìµœì í™” (ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ í•˜ì—ì„œ ë¯¼ê°ë„ ìµœëŒ€í™”)"""
        
        # í™•ë¥  ì˜ˆì¸¡
        if model_type == 'XGBoost_Balanced':
            dval = xgb.DMatrix(X_val)
            y_proba = model.predict(dval)
        else:  # LightGBM
            y_proba = model.predict(X_val)
        
        # ë‹¤ì–‘í•œ ì„ê³„ê°’ í…ŒìŠ¤íŠ¸
        thresholds = np.arange(0.05, 0.95, 0.05)
        
        best_threshold = 0.5
        best_sensitivity = 0
        valid_thresholds = []
        
        print(f"     ğŸ¯ ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ í•˜ì—ì„œ ë¯¼ê°ë„ ìµœëŒ€í™”")
        
        # ëª¨ë“  ì„ê³„ê°’ì— ëŒ€í•´ ì •ë°€ë„ì™€ ë¯¼ê°ë„ ê³„ì‚°
        for threshold in thresholds:
            y_pred_thresh = (y_proba >= threshold).astype(int)
            sensitivity = recall_score(y_val, y_pred_thresh, zero_division=0)
            precision = precision_score(y_val, y_pred_thresh, zero_division=0)
            f1 = f1_score(y_val, y_pred_thresh, zero_division=0)
            
            # ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì„ê³„ê°’ë“¤ë§Œ ê³ ë ¤
            if precision >= 0.8:
                valid_thresholds.append({
                    'threshold': threshold,
                    'sensitivity': sensitivity,
                    'precision': precision,
                    'f1': f1
                })
        
        # ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì„ê³„ê°’ë“¤ ì¤‘ì—ì„œ ë¯¼ê°ë„ ìµœëŒ€í™”
        if valid_thresholds:
            best_result = max(valid_thresholds, key=lambda x: x['sensitivity'])
            best_threshold = best_result['threshold']
            best_sensitivity = best_result['sensitivity']
            print(f"       âœ… ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ ë§Œì¡±: {len(valid_thresholds)}ê°œ í›„ë³´ ì¤‘ ë¯¼ê°ë„ ìµœëŒ€ ì„ íƒ")
            print(f"       ğŸ“Š ì„ íƒëœ ì„±ëŠ¥: ë¯¼ê°ë„ {best_result['sensitivity']:.4f}, ì •ë°€ë„ {best_result['precision']:.4f}")
        else:
            # ì •ë°€ë„ â‰¥ 0.8 ë¶ˆê°€ëŠ¥í•œ ê²½ìš° F1 ì ìˆ˜ ìµœëŒ€í™”ë¡œ fallback
            print(f"       âš ï¸ ì •ë°€ë„ â‰¥ 0.8 ë¶ˆê°€ëŠ¥ â†’ F1 ì ìˆ˜ ìµœëŒ€í™”ë¡œ fallback")
            fallback_results = []
            for threshold in thresholds:
                y_pred_thresh = (y_proba >= threshold).astype(int)
                sensitivity = recall_score(y_val, y_pred_thresh, zero_division=0)
                precision = precision_score(y_val, y_pred_thresh, zero_division=0)
                f1 = f1_score(y_val, y_pred_thresh, zero_division=0)
                fallback_results.append({
                    'threshold': threshold,
                    'sensitivity': sensitivity,
                    'precision': precision,
                    'f1': f1
                })
            
            if fallback_results:
                best_result = max(fallback_results, key=lambda x: x['f1'])
                best_threshold = best_result['threshold']
                best_sensitivity = best_result['sensitivity']
                print(f"       ğŸ“Š Fallback ì„±ëŠ¥: ë¯¼ê°ë„ {best_result['sensitivity']:.4f}, ì •ë°€ë„ {best_result['precision']:.4f}")
        
        return best_threshold, best_sensitivity
    
    def train_mold_specific_models(self, df_train):
        """ê° mold_codeë³„ ìµœì  ëª¨ë¸ í›ˆë ¨"""
        
        print(f"\nğŸ¤– ê° mold_codeë³„ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        # íƒ€ê²Ÿê³¼ íŠ¹ì„± ë¶„ë¦¬
        X = df_train.drop(['passorfail', 'mold_code'], axis=1)
        y = df_train['passorfail']
        self.feature_names = X.columns.tolist()
        
        print(f"   ğŸ“‹ ì‚¬ìš© ë³€ìˆ˜: {len(self.feature_names)}ê°œ (EMS_operation_time ì œì™¸)")
        
        # ê° mold_codeë³„ë¡œ í›ˆë ¨
        for mold_code in self.best_model_config.keys():
            if mold_code not in df_train['mold_code'].values:
                print(f"âš ï¸ mold_code {mold_code} ë°ì´í„° ì—†ìŒ")
                continue
            
            print(f"\nğŸ” === mold_code {mold_code} ëª¨ë¸ í›ˆë ¨ ===")
            
            # í•´ë‹¹ mold_code ë°ì´í„° ì¶”ì¶œ
            mold_mask = df_train['mold_code'] == mold_code
            X_mold = X[mold_mask]
            y_mold = y[mold_mask]
            
            print(f"   - ë°ì´í„°: {len(X_mold)}ê°œ, ë¶ˆëŸ‰ë¥ : {y_mold.mean():.4f}")
            
            # ë°ì´í„° ë¶„í• 
            X_train_mold, X_val_mold, y_train_mold, y_val_mold = train_test_split(
                X_mold, y_mold, test_size=0.2, random_state=42, stratify=y_mold
            )
            
            # ëª¨ë¸ íƒ€ì… í™•ì¸
            model_type = self.best_model_config[mold_code]
            print(f"   - ìµœì  ëª¨ë¸: {model_type}")
            
            # ëª¨ë¸ë³„ ìì²´ íŠœë‹
            if model_type == 'XGBoost_Balanced':
                best_model, best_params = self.optimize_xgboost(X_train_mold, y_train_mold, X_val_mold, y_val_mold)
            else:  # LightGBM_Balanced
                best_model, best_params = self.optimize_lightgbm(X_train_mold, y_train_mold, X_val_mold, y_val_mold)
            
            # ì„ê³„ê°’ ìµœì í™”
            print(f"   - ì„ê³„ê°’ ìµœì í™” ì¤‘...")
            best_threshold, best_score = self.optimize_threshold(
                best_model, X_val_mold, y_val_mold, model_type, mold_code
            )
            
            # ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
            if model_type == 'XGBoost_Balanced':
                dval = xgb.DMatrix(X_val_mold)
                y_proba = best_model.predict(dval)
            else:
                y_proba = best_model.predict(X_val_mold)
            
            y_pred_optimal = (y_proba >= best_threshold).astype(int)
            
            sensitivity = recall_score(y_val_mold, y_pred_optimal, zero_division=0)
            f1 = f1_score(y_val_mold, y_pred_optimal, zero_division=0)
            accuracy = accuracy_score(y_val_mold, y_pred_optimal)
            
            print(f"   âœ… íŠœë‹ ì™„ë£Œ:")
            print(f"     * ìµœì  ì„ê³„ê°’: {best_threshold:.3f}")
            print(f"     * ìµœì¢… ì„±ëŠ¥: ë¯¼ê°ë„ {sensitivity:.4f}, F1 {f1:.4f}, ì •í™•ë„ {accuracy:.4f}")
            print(f"     * ìµœì í™” ì ìˆ˜: {best_score:.4f}")
            
            # ì „ì²´ ë°ì´í„°ë¡œ ì¬í›ˆë ¨
            if model_type == 'XGBoost_Balanced':
                dtrain_full = xgb.DMatrix(X_mold, label=y_mold)
                final_model = xgb.train(best_params, dtrain_full, num_boost_round=best_model.best_iteration)
            else:
                train_data_full = lgb.Dataset(X_mold, label=y_mold)
                final_model = lgb.train(best_params, train_data_full, num_boost_round=best_model.best_iteration)
            
            # ëª¨ë¸ ë° ì„ê³„ê°’ ì €ì¥
            self.models[mold_code] = final_model
            self.best_params[mold_code] = best_params
            self.optimal_thresholds[mold_code] = best_threshold
        
        print(f"\nâœ… ëª¨ë“  mold_code ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(self.models)}ê°œ")
        print(f"   - í‰ê·  ì„ê³„ê°’: {np.mean(list(self.optimal_thresholds.values())):.3f}")
        print(f"   ğŸ¯ ëª¨ë“  ê¸ˆí˜•: ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ í•˜ì—ì„œ ë¯¼ê°ë„ ìµœëŒ€í™”")
    
    def predict(self, df_test):
        """í†µí•© ì˜ˆì¸¡ í•¨ìˆ˜ (ì„ê³„ê°’ ì ìš©)"""
        
        print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘")
        
        # ì „ì²˜ë¦¬
        df_processed, tryshot_data, tryshot_predictions = self.preprocess_data(df_test, is_training=False)
        
        if len(df_processed) == 0:
            print("âš ï¸ ì˜ˆì¸¡í•  ë°ì´í„° ì—†ìŒ (ëª¨ë‘ tryshot)")
            return tryshot_predictions
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        predictions = pd.Series(index=df_processed.index, dtype=int, name='prediction')
        
        # ê° mold_codeë³„ ì˜ˆì¸¡
        for mold_code in df_processed['mold_code'].unique():
            if mold_code not in self.models:
                print(f"âš ï¸ mold_code {mold_code} ëª¨ë¸ ì—†ìŒ - ê¸°ë³¸ê°’(0) ì˜ˆì¸¡")
                mask = df_processed['mold_code'] == mold_code
                predictions[mask] = 0
                continue
            
            # í•´ë‹¹ mold_code ë°ì´í„° ì¶”ì¶œ
            mask = df_processed['mold_code'] == mold_code
            X_mold = df_processed[mask].drop(['passorfail', 'mold_code'], axis=1)
            
            # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            model = self.models[mold_code]
            model_type = self.best_model_config[mold_code]
            threshold = self.optimal_thresholds.get(mold_code, 0.5)
            
            # í™•ë¥  ì˜ˆì¸¡ ë° ì„ê³„ê°’ ì ìš©
            if model_type == 'XGBoost_Balanced':
                dtest = xgb.DMatrix(X_mold)
                y_proba = model.predict(dtest)
            else:  # LightGBM
                y_proba = model.predict(X_mold)
            
            # ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ì˜ˆì¸¡
            pred = (y_proba >= threshold).astype(int)
            predictions[mask] = pred
            
            print(f"   - mold_code {mold_code}: {mask.sum()}ê°œ ì˜ˆì¸¡ ì™„ë£Œ (ì„ê³„ê°’: {threshold:.3f})")
        
        # tryshot ì˜ˆì¸¡ê³¼ í•©ì¹˜ê¸°
        if tryshot_predictions is not None:
            all_predictions = pd.concat([predictions, tryshot_predictions])
            print(f"   - tryshot ê°•ì œ ë¶ˆëŸ‰: {len(tryshot_predictions)}ê°œ")
        else:
            all_predictions = predictions
        
        print(f"âœ… ì „ì²´ ì˜ˆì¸¡ ì™„ë£Œ: {len(all_predictions)}ê°œ")
        
        return all_predictions
    
    def save_pipeline(self, filepath):
        """íŒŒì´í”„ë¼ì¸ ì €ì¥"""
        pipeline_data = {
            'models': self.models,
            'best_params': self.best_params,
            'optimal_thresholds': self.optimal_thresholds,
            'feature_names': self.feature_names,
            'best_model_config': self.best_model_config,
            'le_working': getattr(self, 'le_working', None),
            'knn_imputer': getattr(self, 'knn_imputer', None),
            'production_mean': getattr(self, 'production_mean', None)
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(pipeline_data, f)
        
        print(f"âœ… íŒŒì´í”„ë¼ì¸ ì €ì¥: {filepath}")
        print(f"   - ëª¨ë¸: {len(self.models)}ê°œ")
        print(f"   - ì„ê³„ê°’: {len(self.optimal_thresholds)}ê°œ")
    
    def load_pipeline(self, filepath):
        """íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        with open(filepath, 'rb') as f:
            pipeline_data = pickle.load(f)
        
        self.models = pipeline_data['models']
        self.best_params = pipeline_data['best_params']
        self.optimal_thresholds = pipeline_data.get('optimal_thresholds', {})
        self.feature_names = pipeline_data['feature_names']
        self.best_model_config = pipeline_data['best_model_config']
        self.le_working = pipeline_data.get('le_working')
        self.knn_imputer = pipeline_data.get('knn_imputer')
        self.production_mean = pipeline_data.get('production_mean')
        
        print(f"âœ… íŒŒì´í”„ë¼ì¸ ë¡œë“œ: {filepath}")
        print(f"   - ëª¨ë¸: {len(self.models)}ê°œ")
        print(f"   - ì„ê³„ê°’: {len(self.optimal_thresholds)}ê°œ")

# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    print("\nğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ")
    
    # ë°ì´í„° ë¡œë“œ
    try:
        df_raw = pd.read_csv('../data/train.csv')
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {df_raw.shape[0]}í–‰, {df_raw.shape[1]}ê°œ ë³€ìˆ˜")
    except:
        print("âŒ train.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()
    
    print("\nğŸ”„ 2ë‹¨ê³„: Train/Test ë¶„í•  (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)")
    
    # tryshot_signal NaN í•„í„°ë§ (ë¶„í•  ì „ì— ë¯¸ë¦¬)
    df_clean = df_raw[df_raw['tryshot_signal'].isna()].copy()
    print(f"âœ… tryshot_signal NaN í•„í„°ë§: {len(df_clean)}í–‰")
    
    # Train/Test ë¶„í•  (ì „ì²˜ë¦¬ ì „ì—!)
    train_df, test_df = train_test_split(
        df_clean, test_size=0.2, random_state=42, 
        stratify=df_clean['passorfail']
    )
    print(f"   - í›ˆë ¨ ë°ì´í„°: {len(train_df)}í–‰")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}í–‰")
    
    print("\nğŸ­ 3ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ í›ˆë ¨")
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„± ë° í›ˆë ¨
    pipeline = MoldCodeQualityPipeline()
    
    # í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬
    train_processed, _, _ = pipeline.preprocess_data(train_df, is_training=True)
    
    # ê° mold_codeë³„ ëª¨ë¸ í›ˆë ¨
    pipeline.train_mold_specific_models(train_processed)
    
    print("\nğŸ§ª 4ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    test_predictions = pipeline.predict(test_df)
    
    # ì„±ëŠ¥ í‰ê°€
    test_processed, _, _ = pipeline.preprocess_data(test_df, is_training=False)
    y_true = test_processed['passorfail']
    y_pred = test_predictions[test_processed.index]
    
    # ì „ì²´ ì„±ëŠ¥
    overall_sensitivity = recall_score(y_true, y_pred, zero_division=0)
    overall_f1 = f1_score(y_true, y_pred, zero_division=0)
    overall_accuracy = accuracy_score(y_true, y_pred)
    
    print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥:")
    print(f"   - ë¯¼ê°ë„: {overall_sensitivity:.4f}")
    print(f"   - F1 ì ìˆ˜: {overall_f1:.4f}")
    print(f"   - ì •í™•ë„: {overall_accuracy:.4f}")
    
    # mold_codeë³„ ì„±ëŠ¥
    print(f"\nğŸ“Š mold_codeë³„ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë° í˜¼ë™í–‰ë ¬:")
    for mold_code in test_processed['mold_code'].unique():
        if mold_code in pipeline.models:
            mask = test_processed['mold_code'] == mold_code
            y_true_mold = y_true[mask]
            y_pred_mold = y_pred[mask]
            
            sensitivity = recall_score(y_true_mold, y_pred_mold, zero_division=0)
            f1 = f1_score(y_true_mold, y_pred_mold, zero_division=0)
            accuracy = accuracy_score(y_true_mold, y_pred_mold)
            precision = precision_score(y_true_mold, y_pred_mold, zero_division=0)
            threshold = pipeline.optimal_thresholds.get(mold_code, 0.5)
            model_type = pipeline.best_model_config[mold_code]
            
            # í˜¼ë™í–‰ë ¬ ê³„ì‚°
            cm = confusion_matrix(y_true_mold, y_pred_mold)
            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
            
            # FN/FP ë¹„ìœ¨ ê³„ì‚°
            fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
            fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
            
            print(f"\nğŸ” === mold_code {mold_code} ìƒì„¸ ê²°ê³¼ ===")
            print(f"   ğŸ“Š ì„±ëŠ¥: ë¯¼ê°ë„ {sensitivity:.4f}, F1 {f1:.4f}, ì •í™•ë„ {accuracy:.4f}, ì •ë°€ë„ {precision:.4f}")
            print(f"   ğŸ¤– ëª¨ë¸: {model_type}, ì„ê³„ê°’: {threshold:.3f} ({mask.sum()}ê°œ)")
            print(f"   ğŸ“ˆ í˜¼ë™í–‰ë ¬:")
            print(f"           ì˜ˆì¸¡")
            print(f"ì‹¤ì œ    ì •ìƒ   ë¶ˆëŸ‰")
            print(f"ì •ìƒ   {tn:4d}  {fp:4d}")
            print(f"ë¶ˆëŸ‰   {fn:4d}  {tp:4d}")
            print(f"   ğŸš¨ ìœ„í—˜ ë¶„ì„:")
            print(f"     - ë†“ì¹œ ë¶ˆëŸ‰í’ˆ(FN): {fn}ê°œ ({fn_rate:.2%})")
            print(f"     - ì˜ëª» ë¶ˆëŸ‰ íŒì •(FP): {fp}ê°œ ({fp_rate:.2%})")
            
            # ì‹¤ë¬´ í•´ì„
            if fn > 0:
                print(f"     âš ï¸ ê³ ê° í´ë ˆì„ ìœ„í—˜: {fn}ê°œ ë¶ˆëŸ‰í’ˆì´ ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ë¨")
            if fp > 10:
                print(f"     ğŸ’° ìˆ˜ìœ¨ ì†ì‹¤: {fp}ê°œ ì •ìƒí’ˆì´ ë¶ˆëŸ‰ìœ¼ë¡œ ë¶„ë¥˜ë¨")
            if fn == 0 and fp <= 5:
                print(f"     âœ… ë§¤ìš° ì•ˆì „í•œ ì„±ëŠ¥!")
            elif fn <= 2 and sensitivity >= 0.95:
                print(f"     ğŸ‘ ìš°ìˆ˜í•œ ì„±ëŠ¥!")
    
    if pipeline.optimal_thresholds:
        thresholds_list = list(pipeline.optimal_thresholds.values())
        print(f"\nğŸ¯ ì„ê³„ê°’ ìµœì í™” ìš”ì•½:")
        print(f"   - í‰ê·  ì„ê³„ê°’: {np.mean(thresholds_list):.3f}")
        print(f"   - ì„ê³„ê°’ ë²”ìœ„: {min(thresholds_list):.3f} ~ {max(thresholds_list):.3f}")
        
        # ì„ê³„ê°’ì´ ë‚®ì€ ìˆœì„œëŒ€ë¡œ (ë” ë¯¼ê°í•˜ê²Œ ë¶ˆëŸ‰ íŒì •)
        sorted_thresholds = sorted(pipeline.optimal_thresholds.items(), key=lambda x: x[1])
        print(f"   - ê°€ì¥ ë¯¼ê°í•œ ê¸ˆí˜•: mold_code {sorted_thresholds[0][0]} (ì„ê³„ê°’: {sorted_thresholds[0][1]:.3f})")
        print(f"   - ê°€ì¥ ë³´ìˆ˜ì ì¸ ê¸ˆí˜•: mold_code {sorted_thresholds[-1][0]} (ì„ê³„ê°’: {sorted_thresholds[-1][1]:.3f})")
    
    print("\nğŸ’¾ 5ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ ì €ì¥")
    pipeline.save_pipeline('xgb_lgb_pipeline.pkl')  # hong ì œê±°!
    
    print("\n" + "=" * 80)
    print("ğŸ‰ XGBoost/LightGBM ì „ìš© íŒŒì´í”„ë¼ì¸ ì™„ì„±! (ì •ë°€ë„ ì œì•½ ë²„ì „)")
    print("   âœ… ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ (ë¶„í•  â†’ ì „ì²˜ë¦¬ â†’ íŠœë‹)")
    print("   âœ… tryshot_signal ë¹„ì¦ˆë‹ˆìŠ¤ ë£° ì ìš©")
    print("   âœ… EMS_operation_time ì™„ì „ ì œê±°ë¡œ ì•ˆì •ì„± í™•ë³´")
    print("   âœ… XGBoost/LightGBM ìì²´ íŠœë‹ìœ¼ë¡œ ê³ ì†í™”")
    print("   âœ… ì •ë°€ë„ ì œì•½ ì„ê³„ê°’ ìµœì í™” (ì •ë°€ë„ â‰¥ 0.8)")
    print("   ğŸš€ ìµœì¢… ëª¨ë¸: XGBoost (8412, 8573, 8722) + LightGBM (8600, 8917)")
    print("   ğŸ—‘ï¸ GridSearchCV ì œê±°: ìì²´ íŠœë‹ìœ¼ë¡œ 3ë°° ë¹ ë¦„")
    print("   ğŸ—‘ï¸ SMOTE ì œê±°: ë‚´ì¥ ë¶ˆê· í˜• ì²˜ë¦¬ë¡œ ì•ˆì •ì„± í™•ë³´")
    print("   ğŸ¯ ì„ê³„ê°’ ì „ëµ: ì •ë°€ë„ â‰¥ 0.8 ì¡°ê±´ì—ì„œ ë¯¼ê°ë„ ìµœëŒ€í™”")
    print("   ğŸ’¡ ì‹¤ìš©ì„± í™•ë³´: ì •ìƒí’ˆ ì˜¤ë¶„ë¥˜ ìµœì†Œí™” + ë¶ˆëŸ‰í’ˆ ê²€ì¶œë ¥ ìµœëŒ€í™”")
    print("   ğŸš€ íŒ€í”Œìš© ì™„ì„±ë²„ì „ - ì‹¤ë¬´ íˆ¬ì… ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 80)