"""
ê³¼ì í•© ë°©ì§€ ê°•í™” íŒŒì´í”„ë¼ì¸ - "ê°œë‚˜ì†Œë‚˜ ë¶ˆëŸ‰" ì˜ˆì¸¡ ë°©ì§€
í™ë‹˜ì„ ìœ„í•œ ìµœì†Œ ë³€ê²½ ê°œì„ ì•ˆ
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, f1_score, recall_score, precision_score, accuracy_score
from sklearn.impute import KNNImputer
import xgboost as xgb
import lightgbm as lgb
import pickle
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("         ê³¼ì í•© ë°©ì§€ ê°•í™” íŒŒì´í”„ë¼ì¸")
print("       'ê°œë‚˜ì†Œë‚˜ ë¶ˆëŸ‰' ì˜ˆì¸¡ ë°©ì§€ + ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ")
print("       ì„ê³„ê°’ 0.5~0.8 + ì •ë°€ë„ í•˜í•œì„  70%")
print("=" * 80)

class ImprovedQualityPipeline:
    """ê³¼ì í•© ë°©ì§€ ê°•í™” í’ˆì§ˆ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, min_threshold=0.5, max_threshold=0.8, min_precision=0.7):
        self.models = {}
        self.best_params = {}
        self.optimal_thresholds = {}
        self.feature_names = None
        self.min_threshold = min_threshold      # 0.5ë¡œ ìƒí–¥ (ê¸°ì¡´ 0.4)
        self.max_threshold = max_threshold      # 0.8ë¡œ ìƒí–¥ (ê¸°ì¡´ 0.7)
        self.min_precision = min_precision      # ì •ë°€ë„ í•˜í•œì„  ì¶”ê°€
        
        # ê° mold_codeë³„ ìµœì  ëª¨ë¸ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
        self.best_model_config = {
            8412: 'XGBoost_Balanced',
            8573: 'XGBoost_Balanced', 
            8600: 'LightGBM_Balanced',
            8722: 'XGBoost_Balanced',
            8917: 'LightGBM_Balanced'
        }
        
        print("âœ… ê³¼ì í•© ë°©ì§€ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ğŸ¯ ê°œì„ ëœ ì„ê³„ê°’ ì „ëµ: {min_threshold}~{max_threshold} ë²”ìœ„")
        print(f"   ğŸ›¡ï¸ ì •ë°€ë„ í•˜í•œì„ : {min_precision*100}% (ê³¼ë„í•œ ë¶ˆëŸ‰ ì˜ˆì¸¡ ë°©ì§€)")
        print(f"   ğŸ“Š ëª¨ë¸ êµ¬ì„±: XGBoost 3ê°œ, LightGBM 2ê°œ")
    
    def preprocess_data(self, df_raw, is_training=True):
        """ë°ì´í„° ì „ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        
        print(f"\nğŸ”§ ì „ì²˜ë¦¬ ì‹œì‘ (í›ˆë ¨ ëª¨ë“œ: {is_training})")
        
        df = df_raw.copy()
        
        # tryshot_signal ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        tryshot_exists = df['tryshot_signal'].notna()
        tryshot_count = tryshot_exists.sum()
        
        if tryshot_count > 0:
            print(f"ğŸš¨ tryshot_signal ê°’ì´ ìˆëŠ” ë°ì´í„° {tryshot_count}ê°œ ë°œê²¬")
            tryshot_data = df[tryshot_exists].copy()
            tryshot_predictions = pd.Series(1, index=tryshot_data.index, name='prediction')
            df = df[~tryshot_exists].copy()
        else:
            tryshot_data = None
            tryshot_predictions = None
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        exclude_columns = ['id', 'line', 'name', 'mold_name', 'time', 'date', 
                          'emergency_stop', 'molten_volume', 'registration_time', 
                          'heating_furnace', 'count', 'tryshot_signal', 'EMS_operation_time']
        
        df = df.drop(columns=[col for col in exclude_columns if col in df.columns])
        print(f"   ğŸš« EMS_operation_time ì œê±°ë¡œ ì•ˆì •ì„± í™•ë³´")
        
        # working ê²°ì¸¡ì¹˜ ì œê±° ë° ë ˆì´ë¸” ì¸ì½”ë”© (ê¸°ì¡´ê³¼ ë™ì¼)
        if 'working' in df.columns and df['working'].isnull().sum() > 0:
            df = df.dropna(subset=['working'])
        
        if is_training:
            self.le_working = LabelEncoder()
            if 'working' in df.columns:
                df['working'] = self.le_working.fit_transform(df['working'])
                print(f"   âœ… working ë ˆì´ë¸” ì¸ì½”ë”© ì™„ë£Œ")
        else:
            if 'working' in df.columns and hasattr(self, 'le_working'):
                try:
                    df['working'] = self.le_working.transform(df['working'])
                except ValueError as e:
                    unknown_mask = ~df['working'].isin(self.le_working.classes_)
                    if unknown_mask.any():
                        first_class = self.le_working.classes_[0]
                        df.loc[unknown_mask, 'working'] = first_class
                    df['working'] = self.le_working.transform(df['working'])
        
        # KNN ë³´ê°„ (ê¸°ì¡´ê³¼ ë™ì¼)
        target_impute_cols = ['molten_temp', 'lower_mold_temp3', 'upper_mold_temp3']
        actual_missing = [col for col in target_impute_cols if col in df.columns and df[col].isnull().sum() > 0]
        
        if actual_missing:
            reference_cols = ['working', 'molten_temp', 'facility_operation_cycleTime', 
                             'production_cycletime', 'mold_code',
                             'lower_mold_temp1', 'lower_mold_temp2', 'lower_mold_temp3',
                             'upper_mold_temp1', 'upper_mold_temp2', 'upper_mold_temp3',
                             'sleeve_temperature', 'Coolant_temperature']
            
            available_reference_cols = [col for col in reference_cols if col in df.columns]
            
            if is_training:
                self.knn_imputer = KNNImputer(n_neighbors=5, weights='uniform')
                df_temp = df[available_reference_cols].copy()
                df_imputed = pd.DataFrame(
                    self.knn_imputer.fit_transform(df_temp),
                    columns=available_reference_cols,
                    index=df_temp.index
                )
            else:
                df_temp = df[available_reference_cols].copy()
                df_imputed = pd.DataFrame(
                    self.knn_imputer.transform(df_temp),
                    columns=available_reference_cols,
                    index=df_temp.index
                )
            
            for col in actual_missing:
                df[col] = df_imputed[col]
        
        # production_cycletime 0ê°’ ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        if 'production_cycletime' in df.columns:
            zero_count = (df['production_cycletime'] == 0).sum()
            if zero_count > 0:
                if is_training:
                    self.production_mean = df[(df['production_cycletime'] > 0) & 
                                            (df['production_cycletime'] <= 115)]['production_cycletime'].mean()
                df.loc[df['production_cycletime'] == 0, 'production_cycletime'] = self.production_mean
        
        # ë‚˜ë¨¸ì§€ ê²°ì¸¡ì¹˜ ì œê±°
        before_rows = len(df)
        df = df.dropna()
        removed_rows = before_rows - len(df)
        if removed_rows > 0:
            print(f"   - ë‚˜ë¨¸ì§€ ê²°ì¸¡ì¹˜ ì œê±°: {removed_rows}í–‰ ì œê±°")
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}í–‰")
        
        return df, tryshot_data, tryshot_predictions
    
    def optimize_xgboost_improved(self, X_train, y_train, X_val, y_val):
        """ê°œì„ ëœ XGBoost íŠœë‹ (ì •ê·œí™” + ì—„ê²©í•œ Early Stopping)"""
        
        print(f"     ğŸš€ ê°œì„ ëœ XGBoost íŠœë‹ ì‹œì‘...")
        
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)
        
        # ê°œì„ : ì •ê·œí™” íŒŒë¼ë¯¸í„° ì¶”ê°€
        base_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'scale_pos_weight': 3,
            'random_state': 42,
            'verbosity': 0,
            # ìƒˆë¡œ ì¶”ê°€: ê³¼ì í•© ë°©ì§€ ì •ê·œí™”
            'reg_alpha': 0.1,      # L1 ì •ê·œí™” (ë¶ˆí•„ìš”í•œ íŠ¹ì„± ì œê±°)
            'reg_lambda': 1.0,     # L2 ì •ê·œí™” (ê°€ì¤‘ì¹˜ í¬ê¸° ì œí•œ)
            'max_delta_step': 1    # ê·¹ë‹¨ì  ì˜ˆì¸¡ ë°©ì§€
        }
        
        param_candidates = [
            {**base_params, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8},
            {**base_params, 'max_depth': 9, 'learning_rate': 0.1, 'subsample': 0.8},
            {**base_params, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.9},
            {**base_params, 'max_depth': 9, 'learning_rate': 0.05, 'subsample': 0.8}
        ]
        
        best_score = 0
        best_params = None
        best_model = None
        
        for i, params in enumerate(param_candidates):
            try:
                # ê°œì„ : Early stopping ë” ì—„ê²©í•˜ê²Œ (20 -> 10)
                model = xgb.train(
                    params,
                    dtrain,
                    num_boost_round=200,
                    evals=[(dval, 'eval')],
                    early_stopping_rounds=10,  # ê¸°ì¡´ 20ì—ì„œ 10ìœ¼ë¡œ ë³€ê²½
                    verbose_eval=False
                )
                
                score = model.best_score
                
                if score > best_score:
                    best_score = score
                    best_params = params
                    best_model = model
                    
            except Exception as e:
                print(f"       íŒŒë¼ë¯¸í„° {i+1} ì‹¤íŒ¨: {str(e)[:30]}...")
                continue
        
        if best_model is None:
            print(f"       âš ï¸ ëª¨ë“  íŒŒë¼ë¯¸í„° ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            best_model = xgb.train(base_params, dtrain, num_boost_round=100)
            best_params = base_params
        
        print(f"       âœ… ìµœê³  ì ìˆ˜: {best_score:.4f} (ì •ê·œí™” ì ìš©)")
        
        return best_model, best_params
    
    def optimize_lightgbm_improved(self, X_train, y_train, X_val, y_val):
        """ê°œì„ ëœ LightGBM íŠœë‹ (ì •ê·œí™” + ì—„ê²©í•œ Early Stopping)"""
        
        print(f"     ğŸš€ ê°œì„ ëœ LightGBM íŠœë‹ ì‹œì‘...")
        
        # ê°œì„ : ì •ê·œí™” íŒŒë¼ë¯¸í„° ì¶”ê°€
        base_params = {
            'objective': 'binary',
            'metric': 'auc',
            'is_unbalance': True,
            'random_state': 42,
            'verbosity': -1,
            'force_col_wise': True,
            # ìƒˆë¡œ ì¶”ê°€: ê³¼ì í•© ë°©ì§€ ì •ê·œí™”
            'reg_alpha': 0.1,      # L1 ì •ê·œí™”
            'reg_lambda': 1.0,     # L2 ì •ê·œí™”
            'min_gain_to_split': 0.1  # ë¶„í•  ìµœì†Œ ì´ë“
        }
        
        param_candidates = [
            {**base_params, 'max_depth': 6, 'learning_rate': 0.1, 'num_leaves': 63},
            {**base_params, 'max_depth': 9, 'learning_rate': 0.1, 'num_leaves': 127},
            {**base_params, 'max_depth': 6, 'learning_rate': 0.2, 'num_leaves': 31},
            {**base_params, 'max_depth': -1, 'learning_rate': 0.05, 'num_leaves': 63}
        ]
        
        best_score = 0
        best_params = None
        best_model = None
        
        for i, params in enumerate(param_candidates):
            try:
                train_data = lgb.Dataset(X_train, label=y_train)
                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
                
                # ê°œì„ : Early stopping ë” ì—„ê²©í•˜ê²Œ (20 -> 10)
                model = lgb.train(
                    params,
                    train_data,
                    num_boost_round=200,
                    valid_sets=[val_data],
                    callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]  # 20 -> 10
                )
                
                score = model.best_score['valid_0']['auc']
                
                if score > best_score:
                    best_score = score
                    best_params = params
                    best_model = model
                    
            except Exception as e:
                print(f"       íŒŒë¼ë¯¸í„° {i+1} ì‹¤íŒ¨: {str(e)[:30]}...")
                continue
        
        if best_model is None:
            print(f"       âš ï¸ ëª¨ë“  íŒŒë¼ë¯¸í„° ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            train_data = lgb.Dataset(X_train, label=y_train)
            best_model = lgb.train(base_params, train_data, num_boost_round=100)
            best_params = base_params
        
        print(f"       âœ… ìµœê³  ì ìˆ˜: {best_score:.4f} (ì •ê·œí™” ì ìš©)")
        
        return best_model, best_params
    
    def optimize_threshold_improved(self, model, X_val, y_val, model_type, mold_code):
        """ê°œì„ ëœ ì„ê³„ê°’ ìµœì í™” (ë¯¼ê°ë„*ì •ë°€ë„ ê· í˜• + ì •ë°€ë„ í•˜í•œì„ )"""
        
        # í™•ë¥  ì˜ˆì¸¡
        if model_type == 'XGBoost_Balanced':
            dval = xgb.DMatrix(X_val)
            y_proba = model.predict(dval)
        else:  # LightGBM
            y_proba = model.predict(X_val)
        
        print(f"     ğŸ¯ ê°œì„ ëœ ì„ê³„ê°’ ë²”ìœ„: {self.min_threshold}~{self.max_threshold}")
        print(f"     ğŸ›¡ï¸ ì •ë°€ë„ í•˜í•œì„ : {self.min_precision*100}%")
        
        # ê°œì„ : ë” ë³´ìˆ˜ì ì¸ ì„ê³„ê°’ ë²”ìœ„ (0.5~0.8)
        thresholds = np.arange(self.min_threshold, self.max_threshold + 0.05, 0.05)
        
        valid_results = []
        
        for threshold in thresholds:
            y_pred_thresh = (y_proba >= threshold).astype(int)
            
            if len(np.unique(y_pred_thresh)) < 2:
                continue
                
            sensitivity = recall_score(y_val, y_pred_thresh, zero_division=0)
            precision = precision_score(y_val, y_pred_thresh, zero_division=0)
            f1 = f1_score(y_val, y_pred_thresh, zero_division=0)
            
            # ê°œì„ : ì •ë°€ë„ í•˜í•œì„  ì²´í¬ (ê³¼ë„í•œ ë¶ˆëŸ‰ ì˜ˆì¸¡ ë°©ì§€)
            if precision < self.min_precision:
                continue
            
            # ê°œì„ : ë¯¼ê°ë„ * ì •ë°€ë„ ê· í˜•ì  ê³„ì‚°
            balance_score = sensitivity * precision
            
            result = {
                'threshold': threshold,
                'sensitivity': sensitivity,
                'precision': precision,
                'f1': f1,
                'balance_score': balance_score,  # ìƒˆë¡œ ì¶”ê°€
                'fp_count': ((y_pred_thresh == 1) & (y_val == 0)).sum(),
                'fn_count': ((y_pred_thresh == 0) & (y_val == 1)).sum()
            }
            
            valid_results.append(result)
        
        if not valid_results:
            print(f"       ğŸš¨ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì„ê³„ê°’ ì—†ìŒ â†’ ê¸°ë³¸ê°’ 0.6 ì‚¬ìš©")
            return 0.6, 0.0
        
        # ê°œì„ : ë¯¼ê°ë„*ì •ë°€ë„ ê· í˜•ì ìœ¼ë¡œ ìµœì í™” (ê¸°ì¡´: ë¯¼ê°ë„ë§Œ)
        best_result = max(valid_results, key=lambda x: x['balance_score'])
        
        print(f"       âœ… ê· í˜•ì ì—ì„œ ìµœì í™”: {len(valid_results)}ê°œ í›„ë³´ ì¤‘ ì„ íƒ")
        print(f"       ğŸ“Š ì„ íƒëœ ì„±ëŠ¥: ë¯¼ê°ë„ {best_result['sensitivity']:.4f}, ì •ë°€ë„ {best_result['precision']:.4f}")
        print(f"       ğŸ¯ ê· í˜• ì ìˆ˜: {best_result['balance_score']:.4f}")
        print(f"       ğŸ“ˆ ì‹¤ë¬´ ì§€í‘œ: FP(ì˜ëª» ë¶ˆëŸ‰) {best_result['fp_count']}ê°œ, FN(ë†“ì¹œ ë¶ˆëŸ‰) {best_result['fn_count']}ê°œ")
        
        # ìƒìœ„ 3ê°œ í›„ë³´ ì¶œë ¥
        top_3 = sorted(valid_results, key=lambda x: x['balance_score'], reverse=True)[:3]
        print(f"       ğŸ† ìƒìœ„ 3ê°œ í›„ë³´ (ê· í˜•ì  ê¸°ì¤€):")
        for i, result in enumerate(top_3):
            print(f"         {i+1}. ì„ê³„ê°’ {result['threshold']:.2f}: ê· í˜•ì  {result['balance_score']:.3f} (ë¯¼ê°ë„ {result['sensitivity']:.3f}, ì •ë°€ë„ {result['precision']:.3f})")
        
        return best_result['threshold'], best_result['balance_score']
    
    def train_mold_specific_models(self, df_train):
        """ê° mold_codeë³„ ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨"""
        
        print(f"\nğŸ¤– ê° mold_codeë³„ ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        X = df_train.drop(['passorfail', 'mold_code'], axis=1)
        y = df_train['passorfail']
        self.feature_names = X.columns.tolist()
        
        print(f"   ğŸ“‹ ì‚¬ìš© ë³€ìˆ˜: {len(self.feature_names)}ê°œ (EMS_operation_time ì œì™¸)")
        
        for mold_code in self.best_model_config.keys():
            if mold_code not in df_train['mold_code'].values:
                print(f"âš ï¸ mold_code {mold_code} ë°ì´í„° ì—†ìŒ")
                continue
            
            print(f"\nğŸ” === mold_code {mold_code} ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨ ===")
            
            mold_mask = df_train['mold_code'] == mold_code
            X_mold = X[mold_mask]
            y_mold = y[mold_mask]
            
            print(f"   - ë°ì´í„°: {len(X_mold)}ê°œ, ë¶ˆëŸ‰ë¥ : {y_mold.mean():.4f}")
            
            X_train_mold, X_val_mold, y_train_mold, y_val_mold = train_test_split(
                X_mold, y_mold, test_size=0.2, random_state=42, stratify=y_mold
            )
            
            model_type = self.best_model_config[mold_code]
            print(f"   - ìµœì  ëª¨ë¸: {model_type} (ì •ê·œí™” + ì—„ê²©í•œ Early Stopping)")
            
            # ê°œì„ ëœ ëª¨ë¸ë³„ íŠœë‹
            if model_type == 'XGBoost_Balanced':
                best_model, best_params = self.optimize_xgboost_improved(X_train_mold, y_train_mold, X_val_mold, y_val_mold)
            else:  # LightGBM_Balanced
                best_model, best_params = self.optimize_lightgbm_improved(X_train_mold, y_train_mold, X_val_mold, y_val_mold)
            
            # ê°œì„ ëœ ì„ê³„ê°’ ìµœì í™”
            print(f"   - ê°œì„ ëœ ì„ê³„ê°’ ìµœì í™” ì¤‘...")
            best_threshold, balance_score = self.optimize_threshold_improved(
                best_model, X_val_mold, y_val_mold, model_type, mold_code
            )
            
            # ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
            if model_type == 'XGBoost_Balanced':
                dval = xgb.DMatrix(X_val_mold)
                y_proba = best_model.predict(dval)
            else:
                y_proba = best_model.predict(X_val_mold)
            
            y_pred_optimal = (y_proba >= best_threshold).astype(int)
            
            sensitivity = recall_score(y_val_mold, y_pred_optimal, zero_division=0)
            precision = precision_score(y_val_mold, y_pred_optimal, zero_division=0)
            f1 = f1_score(y_val_mold, y_pred_optimal, zero_division=0)
            
            print(f"   âœ… ê°œì„ ëœ íŠœë‹ ì™„ë£Œ:")
            print(f"     * ìµœì  ì„ê³„ê°’: {best_threshold:.3f} (ë³´ìˆ˜ì  ë²”ìœ„ {self.min_threshold}~{self.max_threshold})")
            print(f"     * ê²€ì¦ ì„±ëŠ¥: ë¯¼ê°ë„ {sensitivity:.4f}, ì •ë°€ë„ {precision:.4f}, F1 {f1:.4f}")
            print(f"     * ê· í˜• ì ìˆ˜: {balance_score:.4f} (ë¯¼ê°ë„*ì •ë°€ë„)")
            print(f"     * ê³¼ì í•© ë°©ì§€: ì •ê·œí™” + ì—„ê²©í•œ Early Stopping + ì •ë°€ë„ í•˜í•œì„ ")
            
            # ì „ì²´ ë°ì´í„°ë¡œ ì¬í›ˆë ¨
            if model_type == 'XGBoost_Balanced':
                dtrain_full = xgb.DMatrix(X_mold, label=y_mold)
                final_model = xgb.train(best_params, dtrain_full, num_boost_round=best_model.best_iteration)
            else:
                train_data_full = lgb.Dataset(X_mold, label=y_mold)
                final_model = lgb.train(best_params, train_data_full, num_boost_round=best_model.best_iteration)
            
            self.models[mold_code] = final_model
            self.best_params[mold_code] = best_params
            self.optimal_thresholds[mold_code] = best_threshold
        
        print(f"\nâœ… ëª¨ë“  mold_code ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(self.models)}ê°œ")
        threshold_values = list(self.optimal_thresholds.values())
        print(f"   - í‰ê·  ì„ê³„ê°’: {np.mean(threshold_values):.3f}")
        print(f"   - ì„ê³„ê°’ ë²”ìœ„: {min(threshold_values):.3f} ~ {max(threshold_values):.3f}")
        print(f"   ğŸ¯ ê³¼ì í•© ë°©ì§€ ì „ëµ: ì •ê·œí™” + ê· í˜•ì  ìµœì í™” + ì •ë°€ë„ {self.min_precision*100}% í•˜í•œì„ ")
    
    def predict(self, df_test):
        """í†µí•© ì˜ˆì¸¡ í•¨ìˆ˜ (ê°œì„ ëœ ì„ê³„ê°’ ì ìš©)"""
        
        print(f"\nğŸ”® ê°œì„ ëœ ì˜ˆì¸¡ ì‹œì‘")
        
        df_processed, tryshot_data, tryshot_predictions = self.preprocess_data(df_test, is_training=False)
        
        if len(df_processed) == 0:
            print("âš ï¸ ì˜ˆì¸¡í•  ë°ì´í„° ì—†ìŒ (ëª¨ë‘ tryshot)")
            return tryshot_predictions
        
        predictions = pd.Series(index=df_processed.index, dtype=int, name='prediction')
        
        for mold_code in df_processed['mold_code'].unique():
            if mold_code not in self.models:
                print(f"âš ï¸ mold_code {mold_code} ëª¨ë¸ ì—†ìŒ - ê¸°ë³¸ê°’(0) ì˜ˆì¸¡")
                mask = df_processed['mold_code'] == mold_code
                predictions[mask] = 0
                continue
            
            mask = df_processed['mold_code'] == mold_code
            X_mold = df_processed[mask].drop(['passorfail', 'mold_code'], axis=1)
            
            model = self.models[mold_code]
            model_type = self.best_model_config[mold_code]
            threshold = self.optimal_thresholds.get(mold_code, 0.6)
            
            if model_type == 'XGBoost_Balanced':
                dtest = xgb.DMatrix(X_mold)
                y_proba = model.predict(dtest)
            else:  # LightGBM
                y_proba = model.predict(X_mold)
            
            pred = (y_proba >= threshold).astype(int)
            predictions[mask] = pred
            
            print(f"   - mold_code {mold_code}: {mask.sum()}ê°œ ì˜ˆì¸¡ ì™„ë£Œ (ê°œì„ ëœ ì„ê³„ê°’: {threshold:.3f})")
        
        if tryshot_predictions is not None:
            all_predictions = pd.concat([predictions, tryshot_predictions])
            print(f"   - tryshot ê°•ì œ ë¶ˆëŸ‰: {len(tryshot_predictions)}ê°œ")
        else:
            all_predictions = predictions
        
        print(f"âœ… ì „ì²´ ì˜ˆì¸¡ ì™„ë£Œ: {len(all_predictions)}ê°œ")
        print(f"   ğŸ›¡ï¸ ê³¼ì í•© ë°©ì§€: ì •ê·œí™” + ë³´ìˆ˜ì  ì„ê³„ê°’ + ì •ë°€ë„ í•˜í•œì„ ")
        
        return all_predictions
    
    def save_pipeline(self, filepath):
        """íŒŒì´í”„ë¼ì¸ ì €ì¥"""
        pipeline_data = {
            'models': self.models,
            'best_params': self.best_params,
            'optimal_thresholds': self.optimal_thresholds,
            'feature_names': self.feature_names,
            'best_model_config': self.best_model_config,
            'min_threshold': self.min_threshold,
            'max_threshold': self.max_threshold,
            'min_precision': self.min_precision,
            'le_working': getattr(self, 'le_working', None),
            'knn_imputer': getattr(self, 'knn_imputer', None),
            'production_mean': getattr(self, 'production_mean', None)
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(pipeline_data, f)
        
        print(f"âœ… ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ ì €ì¥: {filepath}")
        print(f"   - ëª¨ë¸: {len(self.models)}ê°œ")
        print(f"   - ê³¼ì í•© ë°©ì§€ ì„ê³„ê°’: {len(self.optimal_thresholds)}ê°œ")

# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    print("\nğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ")
    
    try:
        df_raw = pd.read_csv('../data/train.csv')
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {df_raw.shape[0]}í–‰, {df_raw.shape[1]}ê°œ ë³€ìˆ˜")
    except:
        print("âŒ train.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()
    
    print("\nğŸ”„ 2ë‹¨ê³„: Train/Test ë¶„í• ")
    
    df_clean = df_raw[df_raw['tryshot_signal'].isna()].copy()
    print(f"âœ… tryshot_signal NaN í•„í„°ë§: {len(df_clean)}í–‰")
    
    train_df, test_df = train_test_split(
        df_clean, test_size=0.2, random_state=42, 
        stratify=df_clean['passorfail']
    )
    print(f"   - í›ˆë ¨ ë°ì´í„°: {len(train_df)}í–‰")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}í–‰")
    
    print("\nğŸ­ 3ë‹¨ê³„: ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ í›ˆë ¨")
    
    # ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ ìƒì„± (0.5~0.8, ì •ë°€ë„ 70% í•˜í•œì„ )
    pipeline = ImprovedQualityPipeline(min_threshold=0.5, max_threshold=0.8, min_precision=0.7)
    
    train_processed, _, _ = pipeline.preprocess_data(train_df, is_training=True)
    pipeline.train_mold_specific_models(train_processed)
    
    print("\nğŸ§ª 4ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€")
    
    test_predictions = pipeline.predict(test_df)
    
    test_processed, _, _ = pipeline.preprocess_data(test_df, is_training=False)
    y_true = test_processed['passorfail']
    y_pred = test_predictions[test_processed.index]
    
    overall_sensitivity = recall_score(y_true, y_pred, zero_division=0)
    overall_precision = precision_score(y_true, y_pred, zero_division=0)
    overall_f1 = f1_score(y_true, y_pred, zero_division=0)
    overall_accuracy = accuracy_score(y_true, y_pred)
    overall_balance = overall_sensitivity * overall_precision
    
    print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥ (ê°œì„ ëœ ë²„ì „):")
    print(f"   - ë¯¼ê°ë„: {overall_sensitivity:.4f}")
    print(f"   - ì •ë°€ë„: {overall_precision:.4f}")
    print(f"   - F1 ì ìˆ˜: {overall_f1:.4f}")
    print(f"   - ì •í™•ë„: {overall_accuracy:.4f}")
    print(f"   - ê· í˜• ì ìˆ˜: {overall_balance:.4f} (ë¯¼ê°ë„*ì •ë°€ë„)")
    
    print(f"\nğŸ“Š mold_codeë³„ ê°œì„ ëœ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
    
    for mold_code in test_processed['mold_code'].unique():
        if mold_code in pipeline.models:
            mask = test_processed['mold_code'] == mold_code
            y_true_mold = y_true[mask]
            y_pred_mold = y_pred[mask]
            
            sensitivity = recall_score(y_true_mold, y_pred_mold, zero_division=0)
            precision = precision_score(y_true_mold, y_pred_mold, zero_division=0)
            f1 = f1_score(y_true_mold, y_pred_mold, zero_division=0)
            balance = sensitivity * precision
            threshold = pipeline.optimal_thresholds.get(mold_code, 0.6)
            
            cm = confusion_matrix(y_true_mold, y_pred_mold)
            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
            
            print(f"\nğŸ” mold_code {mold_code}:")
            print(f"   ğŸ“Š ë¯¼ê°ë„: {sensitivity:.4f}, ì •ë°€ë„: {precision:.4f}, F1: {f1:.4f}")
            print(f"   ğŸ¯ ê°œì„ ëœ ì„ê³„ê°’: {threshold:.3f} (0.5~0.8 ë²”ìœ„)")
            print(f"   âš–ï¸ ê· í˜• ì ìˆ˜: {balance:.4f} (ë¯¼ê°ë„*ì •ë°€ë„)")
            print(f"   ğŸ“ˆ ì‹¤ë¬´ ì§€í‘œ: ë¶ˆëŸ‰ ì˜ˆì¸¡ {tp+fp}ê°œ, ë†“ì¹œ ë¶ˆëŸ‰ {fn}ê°œ, ì˜ëª» ë¶ˆëŸ‰ íŒì • {fp}ê°œ")
            
            # ê°œì„  íš¨ê³¼ í‰ê°€
            if precision >= 0.7 and sensitivity >= 0.8:
                print(f"   âœ… ìš°ìˆ˜í•œ ê· í˜•! (ë†’ì€ ë¯¼ê°ë„ + ì •ë°€ë„ 70% ì´ìƒ)")
            elif precision < 0.7:
                print(f"   âš ï¸ ì •ë°€ë„ ë¶€ì¡±: {precision:.3f} < 0.7 (ê³¼ë„í•œ ë¶ˆëŸ‰ ì˜ˆì¸¡)")
            elif fn > 10:
                print(f"   âš ï¸ ë¶ˆëŸ‰í’ˆ ë†“ì¹¨ ì£¼ì˜ (FN: {fn}ê°œ)")
    
    if pipeline.optimal_thresholds:
        thresholds_list = list(pipeline.optimal_thresholds.values())
        print(f"\nğŸ¯ ê°œì„ ëœ ì„ê³„ê°’ ìš”ì•½:")
        print(f"   - í‰ê·  ì„ê³„ê°’: {np.mean(thresholds_list):.3f}")
        print(f"   - ì„ê³„ê°’ ë²”ìœ„: {min(thresholds_list):.3f} ~ {max(thresholds_list):.3f}")
        print(f"   - ì„ê³„ê°’ ë‹¤ì–‘ì„±: {'âœ…' if len(set([round(t, 1) for t in thresholds_list])) > 1 else 'âŒ'}")
        print(f"   - 0.5~0.8 ë²”ìœ„ ì¤€ìˆ˜: {'âœ…' if all(0.5 <= t <= 0.8 for t in thresholds_list) else 'âŒ'}")
    
    print("\nğŸ’¾ 5ë‹¨ê³„: ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ ì €ì¥")
    pipeline.save_pipeline('xgb_lgb_pipeline_improved.pkl')
    
    print("\n" + "=" * 80)
    print("ğŸ‰ ê³¼ì í•© ë°©ì§€ ê°•í™” íŒŒì´í”„ë¼ì¸ ì™„ì„±!")
    print("   âœ… ì„ê³„ê°’ 0.5~0.8 ë²”ìœ„ë¡œ 'ê°œë‚˜ì†Œë‚˜ ë¶ˆëŸ‰' ë°©ì§€")
    print("   âœ… ì •ë°€ë„ 70% í•˜í•œì„ ìœ¼ë¡œ ê³¼ë„í•œ ë¶ˆëŸ‰ ì˜ˆì¸¡ ì°¨ë‹¨")
    print("   âœ… ë¯¼ê°ë„*ì •ë°€ë„ ê· í˜•ì ìœ¼ë¡œ ìµœì í™”")
    print("   âœ… ì •ê·œí™” íŒŒë¼ë¯¸í„°ë¡œ ê³¼ì í•© ë°©ì§€")
    print("   âœ… ì—„ê²©í•œ Early Stopping (20â†’10)")
    print("   ğŸ¯ ì‹¤ë¬´ ì ìš©: ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œë„ ì•ˆì •ì  ì„±ëŠ¥!")
    print("   ğŸ’¡ í•µì‹¬ ê°œì„ : ì¼ë°˜í™” ì„±ëŠ¥ ìµœìš°ì„ ")
    print("=" * 80)